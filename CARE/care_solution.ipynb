{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-aware image restoration\n",
    "\n",
    "Fluorescence microscopy is constrained by the microscope's optics, fluorophore chemistry, and the sample's photon tolerance. These constraints require balancing imaging speed, resolution, light exposure, and depth. CARE demonstrates how Deep learning can extend the range of biological phenomena observable by microscopy.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](nb_data/tradeoff.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARE\n",
    "In this first exercise we will train a CARE model for a 2D denoising task. \n",
    "We'll use a UNet model that we defined in the semantic segmentation exercise, so we'd\n",
    "need to import the necessary modules and functions from that module\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](nb_data/img_intro.png) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory actions\n",
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>regression</code> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "\n",
    "unet_spec = importlib.util.spec_from_file_location(\n",
    "    \"segm\", \"/home/igor.zubarev/projects/dl_courses/01_segmentation/unet.py\"\n",
    ")\n",
    "unet_module = importlib.util.module_from_spec(unet_spec)\n",
    "unet_spec.loader.exec_module(unet_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from careamics_portfolio import PortfolioManager\n",
    "import tifffile\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import no_grad, cuda\n",
    "from transforms import augment_batch, normalize, denormalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\"><div class=\"alert alert-block alert-success\"><h1>Checkpoint 1: Prepare data</h1>\n",
    "</div>\n",
    "\n",
    "CARE is a fully supervised algorithm, therefore we'd need image pairs for training .In practice this is best achieved by acquiring 2 interleaved stacks, i.e. different channels that correspond to the different exposure/laser settings.\n",
    "\n",
    "We'll be using high SNR(signal-to-noise ratio) images of Human U2OS cells taken from the Broad Bioimage Benchmark Collection. Low SNR images were created by synthetically adding strong read-out and shot-noise and applying pixel binning of 2x2, thus mimicking acquisitions at a very low light level.\n",
    "\n",
    "Since the image pairs were synthetically created in this example, they are already aligned perfectly. Note that when working with real paired acquisitions, the low and high SNR images are not pixel-perfect aligned so typically need to be co-registered before training a CARE model.\n",
    "\n",
    "To train a denoising network, we will use the same UNet model we used in the semantic segmentation exercise.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Download the data</h3>\n",
    "</div>\n",
    "\n",
    "For downloading the data, we will use the careamics-portfolio package. The package provides a collection of microscopy datasets and convenience functions for downloading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "portfolio = PortfolioManager()\n",
    "print(portfolio.denoising)\n",
    "\n",
    "root_path = Path(\"./data\")\n",
    "files = portfolio.denoising.CARE_U2OS.download(root_path)\n",
    "print(f\"List of downloaded files: {files}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Split the dataset into training and validation</h3>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "root_path = Path(\"./data/denoising-CARE_U2OS.unzip/data/U2OS\")\n",
    "assert root_path.exists(), f\"Path {root_path} does not exist\"\n",
    "\n",
    "train_images_path = root_path / \"train\" / \"low\"\n",
    "train_targets_path = root_path / \"train\" / \"GT\"\n",
    "test_image_path = root_path / \"test\" / \"low\"\n",
    "test_target_path = root_path / \"test\" / \"GT\"\n",
    "\n",
    "\n",
    "image_files = list(Path(train_images_path).rglob(\"*.tif\"))\n",
    "target_files = list(Path(train_targets_path).rglob(\"*.tif\"))\n",
    "assert len(image_files) == len(\n",
    "    target_files\n",
    "), \"Number of images and targets do not match\"\n",
    "\n",
    "print(f\"Total size of train dataset: {len(image_files)}\")\n",
    "\n",
    "# Split the train data into train and validation\n",
    "seed = 42\n",
    "train_files_percentage = 0.8\n",
    "np.random.seed(seed)\n",
    "shuffled_indices = np.random.permutation(len(image_files))\n",
    "image_files = np.array(image_files)[shuffled_indices]\n",
    "target_files = np.array(target_files)[shuffled_indices]\n",
    "assert all(\n",
    "    [i.name == j.name for i, j in zip(image_files, target_files)]\n",
    "), \"Files do not match\"\n",
    "\n",
    "train_image_files = image_files[: int(train_files_percentage * len(image_files))]\n",
    "train_target_files = target_files[: int(train_files_percentage * len(target_files))]\n",
    "val_image_files = image_files[int(train_files_percentage * len(image_files)) :]\n",
    "val_target_files = target_files[int(train_files_percentage * len(target_files)) :]\n",
    "assert all(\n",
    "    [i.name == j.name for i, j in zip(train_image_files, train_target_files)]\n",
    "), \"Train files do not match\"\n",
    "assert all(\n",
    "    [i.name == j.name for i, j in zip(val_image_files, val_target_files)]\n",
    "), \"Val files do not match\"\n",
    "\n",
    "print(f\"Train dataset size: {len(train_image_files)}\")\n",
    "print(f\"Validation dataset size: {len(val_image_files)}\")\n",
    "\n",
    "# Read the test files\n",
    "test_image_files = list(test_image_path.rglob(\"*.tif\"))\n",
    "test_target_files = list(test_target_path.rglob(\"*.tif\"))\n",
    "print(f\"Number of test files: {len(test_image_files)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Task 1.0: Define patching function</h3>\n",
    "</div>\n",
    "\n",
    "In the majority of cases microscopy images are too large to be processed at once and need to be divided into smaller patches. We will define a function that takes an image and divides it into patches of a given size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patches(\n",
    "    image_array: np.ndarray,\n",
    "    target_array: np.ndarray,\n",
    "    patch_size: Union[List[int], Tuple[int, ...]],\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate patches from an array in a random manner.\n",
    "\n",
    "    The method calculates how many patches the image can be divided into and then\n",
    "    extracts an equal number of random patches.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    arr : np.ndarray\n",
    "        Input image array.\n",
    "    patch_size : Tuple[int]\n",
    "        Patch sizes in each dimension.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    Generator[np.ndarray, None, None]\n",
    "        Generator of patches.\n",
    "    \"\"\"\n",
    "    # random generator\n",
    "    rng = np.random.default_rng()\n",
    "    image_patches = []\n",
    "    target_patches = []\n",
    "\n",
    "    # iterate over the number of samplesin the input array\n",
    "    for s in range(image_array.shape[0]):\n",
    "        # calculate the number of patches\n",
    "        sample = image_array[s]\n",
    "        target_sample = target_array[s]\n",
    "        n_patches = np.ceil(np.prod(sample.shape) / np.prod(patch_size)).astype(int)\n",
    "        # iterate over the number of patches\n",
    "        for _ in range(n_patches):\n",
    "            # get crop coordinates\n",
    "            crop_coords = [\n",
    "                rng.integers(0, sample.shape[i] - patch_size[i], endpoint=True)\n",
    "                for i in range(len(patch_size))\n",
    "            ]\n",
    "\n",
    "            # extract patch\n",
    "            patch = (\n",
    "                sample[\n",
    "                    (\n",
    "                        ...,  # type: ignore\n",
    "                        *[  # type: ignore\n",
    "                            slice(c, c + patch_size[i])\n",
    "                            for i, c in enumerate(crop_coords)\n",
    "                        ],\n",
    "                    )\n",
    "                ]\n",
    "                .copy()\n",
    "                .astype(np.float32)\n",
    "            )\n",
    "\n",
    "            # same for target\n",
    "            target_patch = (\n",
    "                target_sample[\n",
    "                    (\n",
    "                        ...,  # type: ignore\n",
    "                        *[  # type: ignore\n",
    "                            slice(c, c + patch_size[i])\n",
    "                            for i, c in enumerate(crop_coords)\n",
    "                        ],\n",
    "                    )\n",
    "                ]\n",
    "                .copy()\n",
    "                .astype(np.float32)\n",
    "            )\n",
    "            # return patch and target patch\n",
    "            image_patches.append(patch)\n",
    "            target_patches.append(target_patch)\n",
    "\n",
    "    return np.stack(image_patches), np.stack(target_patches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Create patches</h3>\n",
    "</div>\n",
    "\n",
    "To train the network, we will use patches of size 128x128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and stack them into arrays\n",
    "train_images_array = np.stack([tifffile.imread(str(f)) for f in train_image_files])\n",
    "train_targets_array = np.stack([tifffile.imread(str(f)) for f in train_target_files])\n",
    "val_images_array = np.stack([tifffile.imread(str(f)) for f in val_image_files])\n",
    "val_targets_array = np.stack([tifffile.imread(str(f)) for f in val_target_files])\n",
    "\n",
    "test_images_array = np.stack([tifffile.imread(str(f)) for f in test_image_files])\n",
    "test_targets_array = np.stack([tifffile.imread(str(f)) for f in test_target_files])\n",
    "\n",
    "\n",
    "print(f\"Train images array shape: {train_images_array.shape}\")\n",
    "print(f\"Validation images array shape: {val_images_array.shape}\")\n",
    "print(f\"Test array shape: {test_images_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and std of the train dataset\n",
    "mean = train_images_array.mean()\n",
    "std = train_images_array.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patches\n",
    "patch_size = (128, 128)\n",
    "\n",
    "train_images_patches, train_targets_patches = generate_patches(\n",
    "    train_images_array, train_targets_array, patch_size\n",
    ")\n",
    "assert (\n",
    "    train_images_patches.shape[0] == train_targets_patches.shape[0]\n",
    "), \"Number of patches do not match\"\n",
    "\n",
    "val_images_patches, val_targets_patches = generate_patches(\n",
    "    val_images_array, val_targets_array, patch_size\n",
    ")\n",
    "assert (\n",
    "    val_images_patches.shape[0] == val_targets_patches.shape[0]\n",
    "), \"Number of patches do not match\"\n",
    "\n",
    "print(f\"Train images patches shape: {train_images_patches.shape}\")\n",
    "print(f\"Validation images patches shape: {val_images_patches.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Visualize training patches</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "ax[0, 0].imshow(train_images_patches[0], cmap=\"magma\")\n",
    "ax[0, 0].set_title(\"Train image\")\n",
    "ax[0, 1].imshow(train_targets_patches[0], cmap=\"magma\")\n",
    "ax[0, 1].set_title(\"Train target\")\n",
    "ax[1, 0].imshow(train_images_patches[1], cmap=\"magma\")\n",
    "ax[1, 0].set_title(\"Train image\")\n",
    "ax[1, 1].imshow(train_targets_patches[1], cmap=\"magma\")\n",
    "ax[1, 1].set_title(\"Train target\")\n",
    "ax[2, 0].imshow(train_images_patches[2], cmap=\"magma\")\n",
    "ax[2, 0].set_title(\"Train image\")\n",
    "ax[2, 1].imshow(train_targets_patches[2], cmap=\"magma\")\n",
    "ax[2, 1].set_title(\"Train target\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Task 1.2: Define a dataset class</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset\n",
    "class CAREDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, image_data: np.ndarray, target_data: np.ndarray, apply_augmentations=False\n",
    "    ):\n",
    "        self.image_data = image_data\n",
    "        self.target_data = target_data\n",
    "        self.patch_augment = apply_augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_data.shape[\n",
    "            0\n",
    "        ]  # Your code here, define the total number of patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Your code here, return the patch and target patch, \n",
    "        # apply augmentations with a condition. Hint: use the augment_batch function\n",
    "        # apply the normalize function to the patch and target patch\n",
    "        # return the patch and target patch. Hint: check the dimensions and the datatype\n",
    "\n",
    "        # get patch\n",
    "        patch = self.image_data[index]\n",
    "\n",
    "        # get target\n",
    "        target = self.target_data[index]\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.patch_augment:\n",
    "            patch, target = augment_batch(patch=patch, target=target)\n",
    "\n",
    "        # Normalize the patch\n",
    "        patch = normalize(patch, mean, std)\n",
    "        target = normalize(target, mean, std)\n",
    "\n",
    "        return patch[np.newaxis].astype(np.float32), target[np.newaxis].astype(\n",
    "            np.float32\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset and create a DataLoader\n",
    "\n",
    "train_dataset = CAREDataset(\n",
    "    image_data=train_images_patches, target_data=train_targets_patches\n",
    ")\n",
    "val_dataset = CAREDataset(\n",
    "    image_data=val_images_patches, target_data=val_targets_patches\n",
    ")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\"><div class=\"alert alert-block alert-success\"><h1>Checkpoint 2: Train a CARE model</h1>\n",
    "</div>\n",
    "\n",
    "Image restoration task is very similar to semantic segmentation task we've done in the previous exercise. We can use the same UNet model for this task."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](nb_data/carenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = unet_module.UNet(depth=3, in_channels=1, out_channels=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Task 2.0: Define the loss function and the optimizer</h3>\n",
    "</div>\n",
    "\n",
    "CARE algorithm uses mean squared error as the loss function. We can use the class `MSE` from pytorch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()  # Your code here, define the loss function, hint: think about the suitable loss function\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=1e-4\n",
    ")  # Your code here, define the optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Task 2.1: Train a model</h3>\n",
    "</div>\n",
    "\n",
    "Here we will train a CARE model using classes and functions you defined in the previous tasks.\n",
    "We're using the same training loop we were using in the semantic segmentation exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "n_epochs = 3\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i, (image_batch, target_batch) in enumerate(train_dataloader):\n",
    "        batch = image_batch.to(device)\n",
    "        target = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        train_loss = loss(output, target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {train_loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    with no_grad():\n",
    "        val_loss = 0\n",
    "        for i, (batch, target) in enumerate(val_dataloader):\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(batch)\n",
    "            val_loss = loss(output, target)\n",
    "\n",
    "        print(f\"Validation loss: {val_loss.item()}\")\n",
    "    \n",
    "    # Save the losses for plotting\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Plot the loss</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train loss\", \"Validation loss\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Task 2.1: Prediction on the test set</h3>\n",
    "</div>\n",
    "\n",
    "Real microscopy images are often too large to be processed in one step, so tiling approach is used to prediction. Note that we call it tiling because it's crucial to reconstruct the image from overlapping tiles to avoid artifacts at the tile boundaries, while during training we used random non-overlapping tiles.\n",
    "For the sake of simplicity, we will use the whole image for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset for the test data\n",
    "\n",
    "test_dataset = CAREDataset(\n",
    "    image_data=test_images_array, target_data=test_targets_array\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction loop\n",
    "\n",
    "test_images = []\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    for i, (image_batch, target_batch) in enumerate(test_dataloader):\n",
    "        image_batch = image_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        output = model(image_batch)\n",
    "\n",
    "        # Save the images and predictions for visualization\n",
    "        test_images.append(denormalize(image_batch.cpu().numpy(), mean, std))\n",
    "        predictions.append(denormalize(output.cpu().numpy(), mean, std))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>Visualize the predictions</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "ax[0, 0].imshow(test_images[0][0].squeeze(), cmap=\"magma\")\n",
    "ax[0, 0].set_title(\"Test image\")\n",
    "ax[0, 1].imshow(predictions[0][0].squeeze(), cmap=\"magma\")\n",
    "ax[0, 1].set_title(\"Prediction\")\n",
    "ax[1, 0].imshow(test_images[1][0].squeeze(), cmap=\"magma\")\n",
    "ax[1, 0].set_title(\"Test image\")\n",
    "ax[1, 1].imshow(predictions[1][0].squeeze(), cmap=\"magma\")\n",
    "ax[1, 1].set_title(\"Prediction\")\n",
    "ax[2, 0].imshow(test_images[2][0].squeeze(), cmap=\"magma\")\n",
    "ax[2, 0].set_title(\"Test image\")\n",
    "ax[2, 1].imshow(predictions[2][0].squeeze(), cmap=\"magma\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmcs_l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
