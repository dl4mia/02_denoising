{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Inference with COSDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we load a trained model and use it to denoise the low signal-to-noise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import tifffile\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "from hub import Hub\n",
    "\n",
    "logger = logging.getLogger('pytorch_lightning')\n",
    "logger.setLevel(logging.WARNING)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load test data\n",
    "The images that we want to denoise are loaded here.  We'll only load 10 images as denoising the entire dataset takes some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_snr_path = \"/group/dl4miacourse/image_regression/penicillium/penicillium_low_snr.tif\"\n",
    "low_snr_path = \"/home/ben/data/penicillium/penicillium_low_snr-smoothed.tif\"\n",
    "low_snr = tifffile.imread(low_snr_path)[:, np.newaxis]\n",
    "low_snr = low_snr[500:510] # Remove this line to denoise an entire dataset\n",
    "low_snr = torch.from_numpy(low_snr.astype(float)).to(torch.float)\n",
    "print(low_snr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with training, data should be a `torch.Tensor` with dimensions: [Number of images, Channels, Z | Y | X] with data type float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Create prediction dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`predict_batch_size` (int): Number of denoised images to produce at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_batch_size = 1\n",
    "\n",
    "predict_set = utils.PredictDataset(low_snr)\n",
    "predict_loader = torch.utils.data.DataLoader(\n",
    "    predict_set,\n",
    "    batch_size=predict_batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Load trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.1.\n",
    "\n",
    "The model we trained in the previous notebook was only trained for 20 minutes. This is long enough to get some denoising results, but a model trained for longer would do better. In the cell below, load the model trained in the previous notebook by recalling the value you gave for `model_name`. Then procede through the notebook to look at how well it performs. \n",
    "\n",
    "Once you reach the end of the notebook, return to this cell to load a model that has been trained for 24 hours by uncommenting line 4, then run the notebook again to see how much difference the extra training time makes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"penicillium\"   ### Insert a string here\n",
    "checkpoint_path = os.path.join(\"checkpoints\", model_name)\n",
    "\n",
    "# checkpoint_path = \"/group/dl4miacourse/image_regression/pretrained_penicillium\" ### Once you reach the bottom of the notebook, return here and uncomment this line to see the pretrained model\n",
    "\n",
    "hub = Hub.load_from_checkpoint(os.path.join(checkpoint_path, \"final_model.ckpt\"))\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\" if use_cuda else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Denoise\n",
    "In this section, we will look at how COSDD does inference. <br>\n",
    "\n",
    "The model denoises images randomly, giving us a different output each time. First, we will compare seven randomly sampled denoised images for the same noisy image. Then, we will produce a single consensus estimate by averaging 100 randomly sampled denoised images. Finally, if the direct denoiser was trained in the previous step, we will see how it can be used to estimate this average in a single pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Random sampling \n",
    "First, we will denoise each image seven times and look at the difference between each estimate. The output of the model is stored in the `samples` variable. This has dimensions [Number of images, Sample index, Channels, Z | Y | X] where different denoised samples for the same image are stored along sample index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 7\n",
    "\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = trainer.predict(hub, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.2.\n",
    "\n",
    "Here, we'll look at the original noisy image and the seven denoised estimates. Change the value for `img_idx` to look at different images and change values for `top`, `bottom`, `left` and `right` to adjust the crop. Use this section to really explore the results. Compare high intensity reigons to low intensity reigons, zoom in and out and spot the differences between the different samples. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmin = np.percentile(low_snr.numpy(), 1)\n",
    "vmax = np.percentile(low_snr.numpy(), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(16, 8))\n",
    "ax[0, 0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0, 0].set_title(\"Input\")\n",
    "for i in range(n_samples):\n",
    "    ax[(i + 1) // 4, (i + 1) % 4].imshow(\n",
    "        samples[img_idx][i][crop], vmin=vmin, vmax=vmax\n",
    "    )\n",
    "    ax[(i + 1) // 4, (i + 1) % 4].set_title(f\"Sample {i+1}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The six sampled denoised images have subtle differences that express the uncertainty involved in this denoising problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 MMSE estimate\n",
    "\n",
    "In the next cell, we sample many denoised images and average them for the minimum mean square estimate (MMSE). The averaged images will be stored in the `MMSEs` variable, which has the same dimensions as `low_snr`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.3.\n",
    "Set `n_samples` to 100 to average 100 images, or a different value to average a different number. Then visually inspeect the results. Examine how the MMSE result differs from the random sample.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = False\n",
    "n_samples = 100   ### Insert an integer here\n",
    "\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "samples = []\n",
    "for _ in tqdm(range(n_samples)):\n",
    "    out = trainer.predict(hub, predict_loader)\n",
    "    out = torch.cat(out, dim=0)\n",
    "    samples.append(out)\n",
    "\n",
    "samples = torch.stack(samples, dim=1)\n",
    "MMSEs = torch.mean(samples, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].imshow(samples[img_idx][0][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1].set_title(\"Sample\")\n",
    "ax[2].imshow(MMSEs[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[2].set_title(\"MMSE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MMSE will usually be closer to the reference than an individual sample and would score a higher PSNR, although it will also be blurrier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 Direct denoising\n",
    "Sampling 100 images and averaging them is a very time consuming. If the direct denoiser was trained in a previous step, it can be used to directly output what the average denoised image would be for a given noisy image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 2.4.\n",
    "\n",
    "Did you enable the direct denoiser in the previous notebook? If so, set `use_direct_denoiser` to `True` to use the Direct Denoiser for inference. If not, go back to section 2.3 to load the pretrained model and return here. \n",
    "\n",
    "Notice how much quicker the direct denoiser is than generating the MMSE results. Visually inspect and explore the results in the same way as before, notice how similar the direct estimate and MMSE estimate are.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_direct_denoiser = True   ### Insert a boolean here\n",
    "hub.direct_pred = use_direct_denoiser\n",
    "\n",
    "direct = trainer.predict(hub, predict_loader)\n",
    "direct = torch.cat(direct, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "top = 0\n",
    "bottom = 1024\n",
    "left = 0\n",
    "right = 1024\n",
    "\n",
    "crop = (0, slice(top, bottom), slice(left, right))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(low_snr[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[0].set_title(\"Input\")\n",
    "ax[1].imshow(direct[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[1].set_title(\"Direct\")\n",
    "ax[2].imshow(MMSEs[img_idx][crop], vmin=vmin, vmax=vmax)\n",
    "ax[2].set_title(\"MMSE\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Incorrect receptive field\n",
    "\n",
    "We've now trained a model and used it to remove structured noise from our data. Before moving onto the next notebook, we'll look at what happens when a COSDD model is trained without considering the noise structures present. \n",
    "\n",
    "COSDD is able to separate imaging noise from clean signal because its autoregressive decoder has a receptive field that spans pixels containing correlated noise, i.e., the row or column of pixels. If its receptive field did not contain pixels with correlated noise, it would not be able to model them and they would be captured by the VAE's latent variables. To demonstrate this, the image below shows a Direct and MMSE estimate of a denoised image where the autoregressive decoder's receptive field was incorrectly set to vertical, leaving it unable to model horizontal noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/penicillium_ynm.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## Checkpoint 2\n",
    "\n",
    "We've completed the process of training and applying a COSDD model for denoising, but there's still more it can do. Continue to the next notebook, generation.ipynb, to see how the model of the data can be used to generate new clean and noisy images.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
