{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-aware image restoration\n",
    "\n",
    "Fluorescence microscopy is constrained by the microscope's optics, fluorophore chemistry, and the sample's photon tolerance. These constraints require balancing imaging speed, resolution, light exposure, and depth. CARE demonstrates how Deep learning can extend the range of biological phenomena observable by microscopy when any of these factor becomes limiting.\n",
    "\n",
    "**Reference**: Weigert, et al. \"Content-aware image restoration: pushing the limits of fluorescence microscopy.\" Nature methods 15.12 (2018): 1090-1097. doi:[10.1038/s41592-018-0216-7](https://www.nature.com/articles/s41592-018-0216-7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CARE\n",
    "\n",
    "In this first exercise we will train a CARE model for a 2D denoising task. CARE stands for Content-Aware image REstoration, and is a supervised method in which we use pairs of degraded and high quality image to train a particular task. The original paper demonstrated improvement of image quality on a variety of tasks such as image restoration or resolution improvement. Here, we will apply CARE to denoise images acquired at low laser power in order to recover the biological structures present in the data!\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"nb_data/img_intro.png\" alt=\"Denoising task\" class=\"center\"> \n",
    "</p>\n",
    "\n",
    "We'll use the UNet model that we built in the semantic segmentation exercise and use a different set of functions to train the model for restoration rather than segmentation.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\"><h3>Objectives</h3>\n",
    "    \n",
    "- Train a UNet on a new task!\n",
    "- Understand how to train CARE\n",
    "  \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mandatory actions\n",
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>02_regression</code> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "from careamics_portfolio import PortfolioManager\n",
    "import tifffile\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import no_grad, cuda\n",
    "from transforms import augment_batch, normalize, denormalize\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 1: Set-up the data\n",
    "\n",
    "CARE is a fully supervised algorithm, therefore we need image pairs for training. In practice this is best achieved by acquiring each image twice, once with short exposure time or low laser power to obtain a noisy low-SNR (signal-to-noise ratio) image, and once with high SNR.\n",
    "\n",
    "Here, we will be using high SNR images of Human U2OS cells taken from the Broad Bioimage Benchmark Collection ([BBBC006v1](https://bbbc.broadinstitute.org/BBBC006)). The low SNR images were created by synthetically adding strong read-out and shot noise, and applying pixel binning of 2x2, thus mimicking acquisitions at a very low light level.\n",
    "\n",
    "Since the image pairs were synthetically created in this example, they are already aligned perfectly. Note that when working with real paired acquisitions, the low and high SNR images are not pixel-perfect aligned so they would often need to be co-registered before training a CARE model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    "To download the data, we use the careamics-portfolio package. The package provides a collection of microscopy datasets and convenience functions for downloading them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data\n",
    "portfolio = PortfolioManager()\n",
    "print(portfolio.denoising)\n",
    "\n",
    "root_path = Path(\"/localscratch/data\")\n",
    "files = portfolio.denoising.CARE_U2OS.download(root_path)\n",
    "print(f\"Number of files downloaded: {len(files)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into training and validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "root_path = root_path / \"denoising-CARE_U2OS.unzip\" / \"data\" / \"U2OS\"\n",
    "assert root_path.exists(), f\"Path {root_path} does not exist\"\n",
    "\n",
    "train_images_path = root_path / \"train\" / \"low\"\n",
    "train_targets_path = root_path / \"train\" / \"GT\"\n",
    "test_image_path = root_path / \"test\" / \"low\"\n",
    "test_target_path = root_path / \"test\" / \"GT\"\n",
    "\n",
    "\n",
    "image_files = list(Path(train_images_path).rglob(\"*.tif\"))\n",
    "target_files = list(Path(train_targets_path).rglob(\"*.tif\"))\n",
    "assert len(image_files) == len(\n",
    "    target_files\n",
    "), \"Number of images and targets do not match\"\n",
    "\n",
    "print(f\"Total size of train dataset: {len(image_files)}\")\n",
    "\n",
    "# Split the train data into train and validation\n",
    "seed = 42\n",
    "train_files_percentage = 0.8\n",
    "np.random.seed(seed)\n",
    "shuffled_indices = np.random.permutation(len(image_files))\n",
    "image_files = np.array(image_files)[shuffled_indices]\n",
    "target_files = np.array(target_files)[shuffled_indices]\n",
    "assert all(\n",
    "    [i.name == j.name for i, j in zip(image_files, target_files)]\n",
    "), \"Files do not match\"\n",
    "\n",
    "train_image_files = image_files[: int(train_files_percentage * len(image_files))]\n",
    "train_target_files = target_files[: int(train_files_percentage * len(target_files))]\n",
    "val_image_files = image_files[int(train_files_percentage * len(image_files)) :]\n",
    "val_target_files = target_files[int(train_files_percentage * len(target_files)) :]\n",
    "assert all(\n",
    "    [i.name == j.name for i, j in zip(train_image_files, train_target_files)]\n",
    "), \"Train files do not match\"\n",
    "assert all(\n",
    "    [i.name == j.name for i, j in zip(val_image_files, val_target_files)]\n",
    "), \"Val files do not match\"\n",
    "\n",
    "print(f\"Train dataset size: {len(train_image_files)}\")\n",
    "print(f\"Validation dataset size: {len(val_image_files)}\")\n",
    "\n",
    "# Read the test files\n",
    "test_image_files = list(test_image_path.rglob(\"*.tif\"))\n",
    "test_target_files = list(test_target_path.rglob(\"*.tif\"))\n",
    "print(f\"Number of test files: {len(test_image_files)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patching function\n",
    "\n",
    "In the majority of cases microscopy images are too large to be processed at once and need to be divided into smaller patches. We will define a function that takes image and target arrays and extract random (paired) patches from them.\n",
    "\n",
    "The method is a bit scary because accessing the whole patch coordinates requires some magical python expressions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(\n",
    "    image_array: np.ndarray,\n",
    "    target_array: np.ndarray,\n",
    "    patch_size: Union[List[int], Tuple[int, ...]],\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create random patches from an array and a target.\n",
    "\n",
    "    The method calculates how many patches the image can be divided into and then\n",
    "    extracts an equal number of random patches.\n",
    "\n",
    "    Important: the images should have an extra dimension before the spatial dimensions.\n",
    "    if you try it with only 2D or 3D images, don't forget to add an extra dimension\n",
    "    using `image = image[np.newaxis, ...]`\n",
    "    \"\"\"\n",
    "    # random generator\n",
    "    rng = np.random.default_rng()\n",
    "    image_patches = []\n",
    "    target_patches = []\n",
    "\n",
    "    # iterate over the number of samples in the input array\n",
    "    for s in range(image_array.shape[0]):\n",
    "        # calculate the number of patches we can extract\n",
    "        sample = image_array[s]\n",
    "        target_sample = target_array[s]\n",
    "        n_patches = np.ceil(np.prod(sample.shape) / np.prod(patch_size)).astype(int)\n",
    "        \n",
    "        # iterate over the number of patches\n",
    "        for _ in range(n_patches):\n",
    "            # get random coordinates for the patch and create the crop coordinates\n",
    "            crop_coords = [\n",
    "                rng.integers(0, sample.shape[i] - patch_size[i], endpoint=True)\n",
    "                for i in range(len(patch_size))\n",
    "            ]\n",
    "\n",
    "            # extract patch from the data\n",
    "            patch = (\n",
    "                sample[\n",
    "                    (\n",
    "                        ...,  \n",
    "                        *[ \n",
    "                            slice(c, c + patch_size[i])\n",
    "                            for i, c in enumerate(crop_coords)\n",
    "                        ],\n",
    "                    )\n",
    "                ]\n",
    "                .copy()\n",
    "                .astype(np.float32)\n",
    "            )\n",
    "\n",
    "            # same for the target patch\n",
    "            target_patch = (\n",
    "                target_sample[\n",
    "                    (\n",
    "                        ...,  \n",
    "                        *[ \n",
    "                            slice(c, c + patch_size[i])\n",
    "                            for i, c in enumerate(crop_coords)\n",
    "                        ],\n",
    "                    )\n",
    "                ]\n",
    "                .copy()\n",
    "                .astype(np.float32)\n",
    "            )\n",
    "            \n",
    "            # add the patch pair to the list\n",
    "            image_patches.append(patch)\n",
    "            target_patches.append(target_patch)\n",
    "    \n",
    "    # return stack of patches\n",
    "    return np.stack(image_patches), np.stack(target_patches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 1: Patch creation</h3>\n",
    "\n",
    "Use a `numpy` array to check that the patching function creates random patches in the image.\n",
    "\n",
    "*hint: you can use `np.arange(5*5).reshape(25)[np.newaxis, ...]` to create ordered arrays and call the patching function on them.*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "image_array = #### YOUR CODE HERE ####\n",
    "target_array = #### YOUR CODE HERE ####\n",
    "\n",
    "image_patches, target_patches = create_patches(image_array, target_array, patch_size=(2, 2))\n",
    "for ind in range(image_patches.shape[0]):\n",
    "    assert (image_patches[ind] == target_patches[ind]).all()\n",
    "    print(image_patches[ind])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create patches\n",
    "\n",
    "To train the network, we will use patches of size 128x128. We first need to load the data, stack it and then call our patching function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and stack them into arrays\n",
    "train_images_array = np.stack([tifffile.imread(str(f)) for f in train_image_files])\n",
    "train_targets_array = np.stack([tifffile.imread(str(f)) for f in train_target_files])\n",
    "val_images_array = np.stack([tifffile.imread(str(f)) for f in val_image_files])\n",
    "val_targets_array = np.stack([tifffile.imread(str(f)) for f in val_target_files])\n",
    "\n",
    "test_images_array = np.stack([tifffile.imread(str(f)) for f in test_image_files])\n",
    "test_targets_array = np.stack([tifffile.imread(str(f)) for f in test_target_files])\n",
    "\n",
    "\n",
    "print(f\"Train images array shape: {train_images_array.shape}\")\n",
    "print(f\"Validation images array shape: {val_images_array.shape}\")\n",
    "print(f\"Test array shape: {test_images_array.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patches\n",
    "patch_size = (128, 128)\n",
    "\n",
    "train_images_patches, train_targets_patches = create_patches(\n",
    "    train_images_array, train_targets_array, patch_size\n",
    ")\n",
    "assert (\n",
    "    train_images_patches.shape[0] == train_targets_patches.shape[0]\n",
    "), \"Number of patches do not match\"\n",
    "\n",
    "val_images_patches, val_targets_patches = create_patches(\n",
    "    val_images_array, val_targets_array, patch_size\n",
    ")\n",
    "assert (\n",
    "    val_images_patches.shape[0] == val_targets_patches.shape[0]\n",
    "), \"Number of patches do not match\"\n",
    "\n",
    "print(f\"Train images patches shape: {train_images_patches.shape}\")\n",
    "print(f\"Validation images patches shape: {val_images_patches.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "ax[0, 0].imshow(train_images_patches[0], cmap=\"magma\")\n",
    "ax[0, 0].set_title(\"Train image\")\n",
    "ax[0, 1].imshow(train_targets_patches[0], cmap=\"magma\")\n",
    "ax[0, 1].set_title(\"Train target\")\n",
    "ax[1, 0].imshow(train_images_patches[1], cmap=\"magma\")\n",
    "ax[1, 0].set_title(\"Train image\")\n",
    "ax[1, 1].imshow(train_targets_patches[1], cmap=\"magma\")\n",
    "ax[1, 1].set_title(\"Train target\")\n",
    "ax[2, 0].imshow(train_images_patches[2], cmap=\"magma\")\n",
    "ax[2, 0].set_title(\"Train image\")\n",
    "ax[2, 1].imshow(train_targets_patches[2], cmap=\"magma\")\n",
    "ax[2, 1].set_title(\"Train target\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class\n",
    "\n",
    "In modern deep learning library, the data is often wrapped into a class called a `Dataset`. Instances of that class are then used to extract the patches before feeding them to the network.\n",
    "\n",
    "Here, the class will be wrapped around our pre-computed stacks of patches. Our `CAREDataset` class is built on top of the PyTorch `Dataset` class (we say it \"inherits\" from `Dataset`, the \"parent\" class). That means that it has some function hidden from us that are defined in the PyTorch repository, but that we also need to implement specific pre-defined methods, such as `__len__` and `__getitem__`. The advantage is that PyTorch knows what to do with a `Dataset` \"child\" class, since its behaviour is defined in the `Dataset` class, but we can do operations that are closely related to our own data in the method we implement.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><h3>Task 1a: Compute the correct statistics</h3>\n",
    "\n",
    "Compute the mean and standard deviation of the training data. We will use these values to normalize the data.\n",
    "\n",
    "Why is normalization important? Should you normalize the input and ground truth data the same way? Do they have similar statistics?\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and std of the train dataset\n",
    "\n",
    "#### YOUR CODE HERE ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 1b: Implement your own Dataset</h3>\n",
    "\n",
    "Implement the `__len__` method so that it returns the total number of patches.\n",
    "\n",
    "*hint: At the end of the next cell, we have some commented code that can help you test the functions you implement*\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><h3>Task 1c: Implement your own Dataset</h3>\n",
    "\n",
    "Implement the `__getitem__` method. It should do the following:\n",
    "\n",
    "1. Extract a patch pair from the stack using the index `index`\n",
    "2. Apply the augmentation to the patch pair if `apply_augmentation` is `True` using `augment_batch(image, target)`\n",
    "3. Normalize the patch pair using the `normalize(array, mean, std)` method\n",
    "4. Return the patch pair as a `float32` image with an additional singleton axis: shape (1, height, width)\n",
    "\n",
    "*hint: for point 4, you can use `array.astype` to set the type of a numpy array to `np.float32`, and you can use `[np.newaxis, ...]` to add a new axis at the beginning.*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset\n",
    "class CAREDataset(Dataset): # CAREDataset inherits from the PyTorch Dataset class\n",
    "    def __init__(\n",
    "        self, image_data: np.ndarray, target_data: np.ndarray, apply_augmentations=False\n",
    "    ):\n",
    "        # these are the \"members\" of the CAREDataset\n",
    "        self.image_data = image_data\n",
    "        self.target_data = target_data\n",
    "        self.patch_augment = apply_augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of patches.\n",
    "        \n",
    "        This method is called when applying `len(...)` to an instance of our class\n",
    "        \"\"\"\n",
    "        return #### YOUR CODE HERE ####\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a single pair of patches.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Your code here, return the patch and target patch, \n",
    "        # apply augmentations with a condition. Hint: use the augment_batch function\n",
    "        # apply the normalize function to the patch and target patch\n",
    "        # return the patch and target patch. Hint: check the dimensions and the datatype\n",
    "\n",
    "        # get patch\n",
    "        patch = #### YOUR CODE HERE ####\n",
    "\n",
    "        # get target\n",
    "        target = #### YOUR CODE HERE ####\n",
    "\n",
    "        # Apply transforms\n",
    "        #### YOUR CODE HERE ####\n",
    "\n",
    "        # Normalize the patch\n",
    "        patch = #### YOUR CODE HERE ####\n",
    "        target = #### YOUR CODE HERE ####\n",
    "\n",
    "        return #### YOUR CODE HERE ####\n",
    "\n",
    "# test the dataset\n",
    "train_dataset = CAREDataset(\n",
    "    image_data=train_images_patches, target_data=train_targets_patches\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Check the Dataset</h3>\n",
    "\n",
    "Run the cell below to make sure your dataset class is working correctly.\n",
    "\n",
    "If any of the tests fail, go back to the previous cell and fix the issue.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the dataset length?\n",
    "assert len(train_dataset) == train_images_patches.shape[0], \"Dataset length is wrong\"\n",
    "\n",
    "# check the normalization\n",
    "assert train_dataset[42][0].max() <= 10, \"Patch isn't normalized properly\"\n",
    "assert train_dataset[42][1].max() <= 10, \"Target patch isn't normalized properly\"\n",
    "\n",
    "# check the get_item function\n",
    "assert train_dataset[42][0].shape == (1, *patch_size), \"Patch size is wrong\"\n",
    "assert train_dataset[42][1].shape == (1, *patch_size), \"Target patch size is wrong\"\n",
    "assert train_dataset[42][0].dtype == np.float32, \"Patch dtype is wrong\"\n",
    "assert train_dataset[42][1].dtype == np.float32, \"Target patch dtype is wrong\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the dataset and create a DataLoader\n",
    "train_dataset = CAREDataset(\n",
    "    image_data=train_images_patches, target_data=train_targets_patches\n",
    ")\n",
    "val_dataset = CAREDataset(\n",
    "    image_data=val_images_patches, target_data=val_targets_patches\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 1: Data</h1>\n",
    "</div>\n",
    "\n",
    "We've created a dataset class that will be used to feed the data to the network. We have also created a patching function that will be used to extract random patches from the images.\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 2: Training the model\n",
    "\n",
    "Image restoration task is very similar to the semantic segmentation task we have done in the previous exercise. We can use the same UNet model and just need to adapt a few things.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](nb_data/carenet.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model\n",
    "\n",
    "We'll be using the model from the previous exercise, so we need to load the relevant module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unet import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = UNet(depth=3, in_channels=1, out_channels=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 2: Loss function</h3>\n",
    "\n",
    "CARE trains image to image, therefore we need a different loss function compared to the segmentation task (image to mask). Can you think of a suitable loss function?\n",
    "\n",
    "*hint: look in the `nn` module of PyTorch ([link](https://pytorch.org/docs/stable/nn.html#loss-functions)).*\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = #### YOUR CODE HERE ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 3: Optimizer</h3>\n",
    "\n",
    "Similarly, define the optimizer and choose the appropriate learning rate. No need to be too inventive here!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = #### YOUR CODE HERE ####"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Here we will train a CARE model using classes and functions you defined in the previous tasks.\n",
    "We're using the same training loop as in the semantic segmentation exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 10\n",
    "device = \"cuda\" if cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# tensorboard\n",
    "tb_logger = SummaryWriter(\"runs/Unet\"+datetime.now().strftime('%d%H-%M%S'))\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for i, (image_batch, target_batch) in enumerate(train_dataloader):\n",
    "        batch = image_batch.to(device)\n",
    "        target = target_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        train_loss = loss(output, target)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {train_loss.item()}\")\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with no_grad():\n",
    "        val_loss = 0\n",
    "        for i, (batch, target) in enumerate(val_dataloader):\n",
    "            batch = batch.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(batch)\n",
    "            val_loss = loss(output, target)\n",
    "\n",
    "        # log tensorboard\n",
    "        step = epoch * len(train_dataloader)\n",
    "        tb_logger.add_scalar(tag=\"train_loss\", scalar_value=train_loss, global_step=step)\n",
    "        tb_logger.add_scalar(tag=\"val_loss\", scalar_value=val_loss, global_step=step)\n",
    "\n",
    "        # we always log the last validation images\n",
    "        tb_logger.add_images(tag=\"val_input\", img_tensor=batch.to(\"cpu\"), global_step=step)\n",
    "        tb_logger.add_images(tag=\"val_target\", img_tensor=target.to(\"cpu\"), global_step=step)\n",
    "        tb_logger.add_images(\n",
    "            tag=\"val_prediction\", img_tensor=output.to(\"cpu\"), global_step=step\n",
    "        )\n",
    "\n",
    "        print(f\"Validation loss: {val_loss.item()}\")\n",
    "\n",
    "    # Save the losses for plotting\n",
    "    train_losses.append(train_loss.item())\n",
    "    val_losses.append(val_loss.item())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train loss\", \"Validation loss\"])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 2: Training</h1>\n",
    "</div>\n",
    "\n",
    "We have defined loss function, optimizer and calculated the mean and standard deviation of the training data. And we've used those to  train the model.\n",
    "\n",
    "<hr style=\"height:2px;\">\n",
    "\n",
    "## Part 3: Predicting on the test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset for the test data\n",
    "test_dataset = CAREDataset(\n",
    "    image_data=test_images_array, target_data=test_targets_array\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>Task 4: Predict using the correct mean/std</h3>\n",
    "\n",
    "Make sure you're using the variables you defined before to get the real pixel values\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction loop\n",
    "\n",
    "test_images = []\n",
    "predictions = []\n",
    "\n",
    "model.eval()\n",
    "with no_grad():\n",
    "    for i, (image_batch, target_batch) in enumerate(test_dataloader):\n",
    "        image_batch = image_batch.to(device)\n",
    "        target_batch = target_batch.to(device)\n",
    "        output = model(image_batch)\n",
    "\n",
    "        # Save the images and predictions for visualization\n",
    "        test_images.append(denormalize(image_batch.cpu().numpy(), #### YOUR CODE HERE ####))\n",
    "        predictions.append(denormalize(output.cpu().numpy(), #### YOUR CODE HERE ####))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "ax[0, 0].imshow(test_images[0][0].squeeze(), cmap=\"magma\")\n",
    "ax[0, 0].set_title(\"Test image\")\n",
    "ax[0, 1].imshow(predictions[0][0].squeeze(), cmap=\"magma\")\n",
    "ax[0, 1].set_title(\"Prediction\")\n",
    "ax[1, 0].imshow(test_images[1][0].squeeze(), cmap=\"magma\")\n",
    "ax[1, 0].set_title(\"Test image\")\n",
    "ax[1, 1].imshow(predictions[1][0].squeeze(), cmap=\"magma\")\n",
    "ax[1, 1].set_title(\"Prediction\")\n",
    "ax[2, 0].imshow(test_images[2][0].squeeze(), cmap=\"magma\")\n",
    "ax[2, 0].set_title(\"Test image\")\n",
    "ax[2, 1].imshow(predictions[2][0].squeeze(), cmap=\"magma\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h1>Checkpoint 3: Predicting</h1>\n",
    "</div>\n",
    "\n",
    "We've used our model to denoise the test dataset and, hopefully, we've seen the clean images !"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
